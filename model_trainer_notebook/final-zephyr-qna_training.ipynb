{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30636,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# # Import the PyTorch library\n# import torch\n\n# # Get the major and minor version of the current CUDA device (GPU)\n# major_version, minor_version = torch.cuda.get_device_capability()\n\n# # Apply the following if the GPU has Ampere or Hopper architecture (RTX 30xx, RTX 40xx, A100, H100, L40, etc.)\n# if major_version >= 8:\n#     # Install the Unsloth library for Ampere and Hopper architecture from GitHub\n#     !pip install \"unsloth[colab_ampere] @ git+https://github.com/unslothai/unsloth.git\" -q\n\n# # Apply the following for older GPUs (V100, Tesla T4, RTX 20xx, etc.)\n# else:\n#     # Install the Unsloth library for older GPUs from GitHub\n#     !pip install \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\" -q\n\n# # Placeholder statement (does nothing)\n# pass\n\n# # Install the Hugging Face Transformers library from GitHub, which allows native 4-bit loading\n# !pip install \"git+https://github.com/huggingface/transformers.git\" -q\n\n# !pip install trl datasets -q","metadata":{"execution":{"iopub.status.busy":"2024-01-28T15:39:03.615747Z","iopub.execute_input":"2024-01-28T15:39:03.616130Z","iopub.status.idle":"2024-01-28T15:39:03.622192Z","shell.execute_reply.started":"2024-01-28T15:39:03.616091Z","shell.execute_reply":"2024-01-28T15:39:03.621220Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install accelerate peft bitsandbytes pip install git+https://github.com/huggingface/transformers trl py7zr auto-gptq optimum\n#%pip install --upgrade datasets huggingface_hub\n# %pip install bitsandbytes optimum auto-gptq trl\n#!pip install \"unsloth[colab] @ git+https://github.com/unslothai/unsloth.git\" -q","metadata":{"execution":{"iopub.status.busy":"2024-01-28T15:39:03.623666Z","iopub.execute_input":"2024-01-28T15:39:03.623970Z","iopub.status.idle":"2024-01-28T15:39:03.635090Z","shell.execute_reply.started":"2024-01-28T15:39:03.623934Z","shell.execute_reply":"2024-01-28T15:39:03.634238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from unsloth import FastLanguageModel\n# Import FastLanguageModel from the Unsloth library\n\n\n\nmax_seq_length = 2048  # Can be set arbitrarily, automatically supports RoPE scaling!\n# Set the maximum sequence length to 2048 (can be changed arbitrarily)\n\ndtype = None  # Automatically detect if None. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n# Set the data type (automatically detect if None, can also specify Float16 or Bfloat16)\n\nload_in_4bit = True  # Reduce memory usage using 4-bit quantization. Can be set to False.\n# Reduce memory usage using 4-bit quantization (can be set to False to disable)\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name=\"unsloth/zephyr-sft-bnb-4bit\",  # Use \"unsloth/mistral-7b\" for 16-bit loading\n    # Load the model \"unsloth/mistral-7b-bnb-4bit\" from pre-training (use \"unsloth/mistral-7b\" for 16-bit loading)\n\n    max_seq_length=max_seq_length,\n    # Set the maximum sequence length\n\n    dtype=dtype,\n    # Set the data type\n\n    load_in_4bit=load_in_4bit,\n    # Apply the settings for 4-bit loading\n\n    # token=\"hf_...\", # Use the token when using a gate model (e.g., meta-llama/Llama-2-7b-hf)\n    # Use Hugging Face's token when using a gate model, or similar cases\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T15:39:03.636050Z","iopub.execute_input":"2024-01-28T15:39:03.636288Z","iopub.status.idle":"2024-01-28T15:39:13.503187Z","shell.execute_reply.started":"2024-01-28T15:39:03.636266Z","shell.execute_reply":"2024-01-28T15:39:13.502415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    # Specify the existing model\n\n    r=16,  # Choose any positive number! Recommended values include 8, 16, 32, 64, 128, etc.\n    # Rank parameter for LoRA. The smaller this value, the fewer parameters will be modified.\n\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                    \"gate_proj\", \"up_proj\", \"down_proj\",],\n    # Specify the modules to which LoRA will be applied\n\n    lora_alpha=32,\n    # Alpha parameter for LoRA. This value determines the strength of the applied LoRA.\n\n    lora_dropout=0,  # Currently, only supports dropout = 0\n    # Dropout rate for LoRA. Currently supports only 0.\n\n    bias=\"none\",  # Currently, only supports bias = \"none\"\n    # Bias usage setting. Currently supports only the setting without bias.\n\n    use_gradient_checkpointing=True,\n    # Whether to use gradient checkpointing to improve memory efficiency\n\n    random_state=3407,\n    # Seed value for random number generation\n\n    max_seq_length=max_seq_length,\n    # Set the maximum sequence length\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T15:39:13.505343Z","iopub.execute_input":"2024-01-28T15:39:13.505771Z","iopub.status.idle":"2024-01-28T15:39:17.066968Z","shell.execute_reply.started":"2024-01-28T15:39:13.505743Z","shell.execute_reply":"2024-01-28T15:39:17.066218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\n{}\n\n### Input:\n{}\n\n### Response:\n{}\"\"\"\n# Define the prompt format for the Alpaca dataset\n\ndef formatting_prompts_func(examples):\n    # Define a function to format each example in the dataset\n\n    instructions = examples[\"instruction\"]\n    inputs       = examples[\"input\"]\n    outputs      = examples[\"output\"]\n    # Get instructions, inputs, and outputs\n\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        # Generate text by combining instructions, inputs, and outputs\n\n        text = alpaca_prompt.format(instruction, input, output)\n        # Format the text according to the prompt format\n\n        texts.append(text)\n    return { \"text\" : texts, }\n    # Return a list of formatted texts\n\npass\n# Placeholder (does nothing)\n\nfrom datasets import load_dataset\n# Import the load_dataset function from the datasets library\n\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n# Load the training data of the cleaned version of the Alpaca dataset from yahma\n\ndataset = dataset.map(formatting_prompts_func, batched=True,)\n# Apply the formatting_prompts_func function to the dataset with batch processing\n\n# shuffled_dataset = dataset.shuffle(seed=42)\n# subset = shuffled_dataset.select(range(20000))\n# dataset=subset","metadata":{"execution":{"iopub.status.busy":"2024-01-28T15:39:17.068107Z","iopub.execute_input":"2024-01-28T15:39:17.068679Z","iopub.status.idle":"2024-01-28T15:39:18.842582Z","shell.execute_reply.started":"2024-01-28T15:39:17.068649Z","shell.execute_reply":"2024-01-28T15:39:18.841615Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\n# Import SFTTrainer from the TRL library\nimport torch\nfrom transformers import TrainingArguments\n# Import TrainingArguments from the Transformers library\n\ntrainer = SFTTrainer(\n    # Initialize the SFTTrainer\n\n    model=model,\n    # Specify the model to be used\n\n    train_dataset=dataset,\n    # Specify the training dataset\n\n    dataset_text_field=\"text\",\n    # Specify the text field in the dataset\n\n    max_seq_length=max_seq_length,\n    # Specify the maximum sequence length\n\n    args=TrainingArguments(\n        # Specify training arguments\n\n        per_device_train_batch_size=32,\n        # Specify the training batch size per device\n\n        gradient_accumulation_steps=1,\n        # Specify the number of steps for gradient accumulation\n\n        warmup_steps=5,\n        # Specify the number of warm-up steps\n\n        max_steps=20,\n        # Specify the maximum number of steps\n\n        learning_rate=2e-4,\n        # Specify the learning rate\n\n        fp16=not torch.cuda.is_bf16_supported(),\n        # Set whether to use 16-bit floating-point precision (fp16)\n\n        bf16=torch.cuda.is_bf16_supported(),\n        # Set whether to use Bfloat16\n\n        logging_steps=1,\n        # Specify the logging steps\n\n        optim=\"adamw_8bit\",\n        # Specify the optimizer (here using 8-bit AdamW)\n\n        weight_decay=0.01,\n        # Specify the weight decay value\n\n        lr_scheduler_type=\"linear\",\n        # Specify the type of learning rate scheduler (linear)\n\n        seed=3407,\n        # Specify the random seed\n\n        output_dir=\"outputs\",\n        # Specify the output directory\n\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-28T15:39:18.844042Z","iopub.execute_input":"2024-01-28T15:39:18.844940Z","iopub.status.idle":"2024-01-28T15:39:19.768013Z","shell.execute_reply.started":"2024-01-28T15:39:18.844901Z","shell.execute_reply":"2024-01-28T15:39:19.767106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pu_stats = torch.cuda.get_device_properties(0)\n# # Get properties of the GPU device at index 0\n\n# start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n# # Get the maximum reserved GPU memory in GB and round to 3 decimal places\n\n# max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n# # Get the total GPU memory in GB and round to 3 decimal places\n\n# print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n# # Display the GPU name and maximum memory\n\n# print(f\"{start_gpu_memory} GB of memory reserved.\")\n# # Display the reserved memory amount","metadata":{"execution":{"iopub.status.busy":"2024-01-28T15:39:19.769613Z","iopub.execute_input":"2024-01-28T15:39:19.770605Z","iopub.status.idle":"2024-01-28T15:39:19.775545Z","shell.execute_reply.started":"2024-01-28T15:39:19.770566Z","shell.execute_reply":"2024-01-28T15:39:19.774563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-01-28T15:39:19.776637Z","iopub.execute_input":"2024-01-28T15:39:19.776897Z","iopub.status.idle":"2024-01-28T16:18:24.082166Z","shell.execute_reply.started":"2024-01-28T15:39:19.776873Z","shell.execute_reply":"2024-01-28T16:18:24.081124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def colab_quantize_to_gguf(save_directory, quantization_method=\"q4_k_m\"):\n    # Define a function for conversion to GGUF\n\n    from transformers.models.llama.modeling_llama import logger\n    import os\n    # Import necessary libraries\n\n    logger.warning_once(\n        \"Unsloth: `colab_quantize_to_gguf` is still in development mode.\\n\"\\\n        \"If anything errors or breaks, please file a ticket on Github.\\n\"\\\n        \"Also, if you used this successfully, please tell us on Discord!\"\n    )\n    # Warn that it's still in development mode and encourage reporting any issues\n\n    # From https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html\n    ALLOWED_QUANTS = \\\n    {\n        # Define currently allowed quantization methods\n        # Including descriptions for each quantization method\n        \"q2_k\"   : \"Uses Q4_K for the attention.vw and feed_forward.w2 tensors, Q2_K for the other tensors.\",\n        \"q3_k_l\" : \"Uses Q5_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\n        \"q3_k_m\" : \"Uses Q4_K for the attention.wv, attention.wo, and feed_forward.w2 tensors, else Q3_K\",\n        \"q3_k_s\" : \"Uses Q3_K for all tensors\",\n        \"q4_0\"   : \"Original quant method, 4-bit.\",\n        \"q4_1\"   : \"Higher accuracy than q4_0 but not as high as q5_0. However has quicker inference than q5 models.\",\n        \"q4_k_m\" : \"Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K\",\n        \"q4_k_s\" : \"Uses Q4_K for all tensors\",\n        \"q5_0\"   : \"Higher accuracy, higher resource usage and slower inference.\",\n        \"q5_1\"   : \"Even higher accuracy, resource usage and slower inference.\",\n        \"q5_k_m\" : \"Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K\",\n        \"q5_k_s\" : \"Uses Q5_K for all tensors\",\n        \"q6_k\"   : \"Uses Q8_K for all tensors\",\n        \"q8_0\"   : \"Almost indistinguishable from float16. High resource use and slow. Not recommended for most users.\",\n    }\n\n    if quantization_method not in ALLOWED_QUANTS.keys():\n        # If the specified quantization method is not allowed, raise an error\n        error = f\"Unsloth: Quant method = [{quantization_method}] not supported. Choose from below:\\n\"\n        for key, value in ALLOWED_QUANTS.items():\n            error += f\"[{key}] => {value}\\n\"\n        raise RuntimeError(error)\n\n    # Display information about the conversion\n    print_info = \\\n        f\"==((====))==  Unsloth: Conversion from QLoRA to GGUF information\\n\"\\\n        f\"   \\\\\\   /|    [0] Installing llama.cpp will take 3 minutes.\\n\"\\\n        f\"O^O/ \\_/ \\\\    [1] Converting HF to GUUF 16bits will take 3 minutes.\\n\"\\\n        f\"\\        /    [2] Converting GGUF 16bits to q4_k_m will take 20 minutes.\\n\"\\\n        f' \"-____-\"     In total, you will have to wait around 26 minutes.\\n'\n    print(print_info)\n    # Display information about the conversion process\n\n    if not os.path.exists(\"llama.cpp\"):\n        # If llama.cpp does not exist, install it\n        print(\"Unsloth: [0] Installing llama.cpp. This will take 3 minutes...\")\n        !git clone https://github.com/ggerganov/llama.cpp\n        !cd llama.cpp && make clean && LLAMA_CUBLAS=1 make -j\n        !pip install gguf protobuf\n        pass\n\n    print(\"Unsloth: Starting conversion from HF to GGUF 16bit...\")\n    # Display that conversion from HF to GGUF 16bit is starting\n    # print(\"Unsloth: [1] Converting HF into GGUF 16bit. This will take 3 minutes...\")\n    !python llama.cpp/convert.py {save_directory} \\\n        --outfile {save_directory}-unsloth.gguf \\\n        --outtype f16\n\n    print(\"Unsloth: Starting conversion from GGUF 16bit to q4_k_m...\")\n    # Display that conversion from GGUF 16bit to the specified quantization method is starting\n    # print(\"Unsloth: [2] Converting GGUF 16bit into q4_k_m. This will take 20 minutes...\")\n    final_location = f\"./{save_directory}-{quantization_method}-unsloth.gguf\"\n    !./llama.cpp/quantize ./{save_directory}-unsloth.gguf \\\n        {final_location} {quantization_method}\n\n    print(f\"Unsloth: Output location: {final_location}\")\n    # Display the output location of the converted file\npass","metadata":{"execution":{"iopub.status.busy":"2024-01-28T16:28:09.049816Z","iopub.execute_input":"2024-01-28T16:28:09.050826Z","iopub.status.idle":"2024-01-28T16:28:09.085861Z","shell.execute_reply.started":"2024-01-28T16:28:09.050786Z","shell.execute_reply":"2024-01-28T16:28:09.084716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login #Model Initialize\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-01-28T16:34:57.677705Z","iopub.execute_input":"2024-01-28T16:34:57.678887Z","iopub.status.idle":"2024-01-28T16:34:57.706737Z","shell.execute_reply.started":"2024-01-28T16:34:57.678846Z","shell.execute_reply":"2024-01-28T16:34:57.705660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"colab_quantize_to_gguf(\"/kaggle/working/output_models/adapter_model.safetensors\", quantization_method=\"q4_k_m\")\n# Convert \"output_model\" to GGUF format. Use the quantization method \"q4_k_m\"","metadata":{"execution":{"iopub.status.busy":"2024-01-28T16:35:45.993271Z","iopub.execute_input":"2024-01-28T16:35:45.993682Z","iopub.status.idle":"2024-01-28T16:36:01.164405Z","shell.execute_reply.started":"2024-01-28T16:35:45.993649Z","shell.execute_reply":"2024-01-28T16:36:01.163110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}